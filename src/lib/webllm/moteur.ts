// src/lib/webllm/moteur.ts
// Service principal pour g√©rer WebLLM - Pattern Singleton

import { CreateMLCEngine, MLCEngine } from "@mlc-ai/web-llm";
import type {
  StatutModele,
  ProgressionChargement,
  ConfigurationModele,
  ParametresGeneration,
  Message,
  ReponseModele,
  ErreurWebLLM
} from './types';
import { text } from "stream/consumers";
import { useStorePersonas } from "../../stroe/storePersonas";


/**
 * Service Singleton pour g√©rer le mod√®le WebLLM
 * 
 * Responsabilit√©s :
 * - Charger le mod√®le une seule fois
 * - G√©rer le statut du mod√®le
 * - G√©n√©rer du texte
 * - G√©rer les erreurs
 * 
 * Pattern Singleton : Une seule instance dans toute l'application
 */
class ServiceMoteurWebLLM {

  private static instance: ServiceMoteurWebLLM | null = null;   // Instance unique du service (Singleton)

  private moteur: MLCEngine | null = null;   // Le moteur WebLLM (null tant qu'il n'est pas charg√©)
 
  private statut: StatutModele = 'inactif';  // Statut actuel du mod√®le
  
  private configuration: ConfigurationModele | null = null; // Configuration du mod√®le actuellement charg√©
  
  // Callbacks pour notifier les changements de statut
  private observateurs: {
    surChangementStatut?: (statut: StatutModele) => void;
    surProgression?: (progression: ProgressionChargement) => void;
    surErreur?: (erreur: ErreurWebLLM) => void;
  } = {};

  /**
   * Constructeur priv√© (Singleton)
   * Ne peut pas √™tre appel√© directement
   */
  private constructor() {
    console.log("ü§ñ Service WebLLM initialis√©");
  }

  /**
   * Obtenir l'instance unique du service (Singleton)
   * 
   * Utilisation :
   *   const service = ServiceMoteurWebLLM.obtenirInstance();
   */
  public static obtenirInstance(): ServiceMoteurWebLLM {
    if (!ServiceMoteurWebLLM.instance) {
      ServiceMoteurWebLLM.instance = new ServiceMoteurWebLLM();
    }
    return ServiceMoteurWebLLM.instance;
  }

  /**
   * Enregistrer des callbacks pour √™tre notifi√© des changements
   * 
   * @param callbacks - Fonctions √† appeler lors d'√©v√©nements
   * 
   * Exemple :
   *   service.enregistrerObservateurs({
   *     surChangementStatut: (statut) => console.log(statut),
   *     surProgression: (prog) => console.log(prog.pourcentage + "%")
   *   });
   */
  public enregistrerObservateurs(callbacks: {
    surChangementStatut?: (statut: StatutModele) => void;
    surProgression?: (progression: ProgressionChargement) => void;
    surErreur?: (erreur: ErreurWebLLM) => void;
  }): void {
    this.observateurs = { ...this.observateurs, ...callbacks };
  }

  /**
   * Obtenir le statut actuel du mod√®le
   * 
   * @returns Le statut actuel ('inactif', 'chargement', 'pret', 'erreur')
   */
  public obtenirStatut(): StatutModele {
    return this.statut;
  }

  /**
   * V√©rifier si le mod√®le est pr√™t √† g√©n√©rer du texte
   * 
   * @returns true si le mod√®le est charg√© et pr√™t
   */
  public estPret(): boolean {
    return this.statut === 'pret' && this.moteur !== null;
  }

  /**
   * Charger le mod√®le WebLLM
   * 
   * @param config - Configuration du mod√®le √† charger
   * 
   * Exemple :
   *   await service.chargerModele({
   *     nom: "Phi-3-mini-4k-instruct-q4f16_1-MLC",
   *     description: "Mod√®le Phi-3 Mini"
   *   });
   */
  public async chargerModele(config: ConfigurationModele): Promise<void> {
    try {
      // 1. V√©rifier si un mod√®le est d√©j√† en cours de chargement
      if (this.statut === 'chargement') {
        console.warn("‚ö†Ô∏è Un mod√®le est d√©j√† en cours de chargement");
        return;
      }

      // 2. Mettre √† jour le statut
      this.changerStatut('chargement');
      this.configuration = config;

      console.log(`üîÑ D√©but du chargement du mod√®le : ${config.nom}`);

      // 3. Cr√©er le moteur WebLLM avec suivi de progression
      this.moteur = await CreateMLCEngine(
        config.nom,
        {
          // Callback appel√© pendant le chargement
          initProgressCallback: (rapport) => {
            console.log(`üìä Progression : ${rapport.text}`);
            
            // Notifier les observateurs
            this.notifierProgression({
              pourcentage: rapport.progress * 100,
              etape: rapport.text
            });
          }
        }
      );

      // 4. Mod√®le charg√© avec succ√®s !
      console.log("‚úÖ Mod√®le charg√© avec succ√®s !");
      this.changerStatut('pret');

    } catch (erreur) {
      // 5. G√©rer les erreurs
      console.error("‚ùå Erreur lors du chargement du mod√®le :", erreur);
      
      const erreurFormatee: ErreurWebLLM = {
        code: 'ERREUR_CHARGEMENT',
        message: "Impossible de charger le mod√®le",
        details: erreur instanceof Error ? erreur.message : String(erreur)
      };

      this.changerStatut('erreur');
      this.notifierErreur(erreurFormatee);
      
      throw erreurFormatee;
    }
  }

  /**
   * G√©n√©rer du texte avec le mod√®le
   * 
   * @param messages - Liste des messages de la conversation
   * @param parametres - Param√®tres de g√©n√©ration (optionnel)
   * @returns Le texte g√©n√©r√©
   */
  /*
  public async genererTexte(
    messages: Message[],
    parametres?: ParametresGeneration,
    onChunk?: (chunk: string) => void,  // <-- Nouveau param√®tre optionnel
  ): Promise<ReponseModele> {
    // 1. V√©rifier que le mod√®le est pr√™t
    if (!this.estPret()) {
      throw {
        code: 'MODELE_NON_PRET',
        message: 'Le mod√®le doit √™tre charg√© avant de g√©n√©rer du texte'
      } as ErreurWebLLM;
    }

    try {
      const tempsDebut = Date.now();

      // 2. Param√®tres par d√©faut si non fournis
      const paramsFinaux: ParametresGeneration = {
        temperature: parametres?.temperature ?? 0.7,
        longueurMaximale: parametres?.longueurMaximale ?? 100,
        topP: parametres?.topP ?? 0.9,
        penaliteFrequence: parametres?.penaliteFrequence ?? 0.0
      };

      // 3. Convertir nos messages au format WebLLM
      // S'assurer que le message syst√®me est toujours en premier
      const systemMessage = messages.find(msg => msg.role === 'system');
      const otherMessages = messages.filter(msg => msg.role !== 'system');
      
      const sortedMessages = systemMessage 
        ? [systemMessage, ...otherMessages]
        : otherMessages;
      
      const messagesWebLLM = sortedMessages.map(msg => ({
        role: msg.role,
        content: msg.contenu
      }));

      console.log("ü§î G√©n√©ration en cours...");

      // 4. G√©n√©rer le texte (mode non-streaming pour l'instant)
      const reponse = await this.moteur!.chat.completions.create({
        messages: messagesWebLLM,
        temperature: paramsFinaux.temperature,
        max_tokens: paramsFinaux.longueurMaximale,
        top_p: paramsFinaux.topP,
        frequency_penalty: paramsFinaux.penaliteFrequence,
        stream: true  // Pas de streaming pour l'instant (on fera √ßa plus tard)


      });

      const tempsFin = Date.now();
      const tempsGeneration = tempsFin - tempsDebut;

      console.log(`‚úÖ Texte g√©n√©r√© en ${tempsGeneration}ms`);
        let texteComplet = "";
        let tokensUtilises = 0;

             // Et il faudrait traiter les chunks
      for await (const chunk of reponse) {
          const nouveauTexte = chunk.choices[0]?.delta?.content || "";
          // Afficher progressivement dans l'UI
          console.log(nouveauTexte);
          texteComplet += nouveauTexte;
          tokensUtilises += chunk.choices[0]?.delta?.content?.length || 0;
        }

      // 5. Retourner la r√©ponse format√©e
      return {
        texte: texteComplet,
        tokensUtilises,
        tempsGeneration
      };

    } catch (erreur) {
      console.error("‚ùå Erreur lors de la g√©n√©ration :", erreur);
      
      throw {
        code: 'ERREUR_GENERATION',
        message: 'Erreur lors de la g√©n√©ration du texte',
        details: erreur instanceof Error ? erreur.message : String(erreur)
      } as ErreurWebLLM;
    }
  }*/


  public async genererTexte(
    messages: Message[],
    parametres?: ParametresGeneration,
    onChunk?: (chunk: string) => void,  // <-- Nouveau param√®tre optionnel
  ): Promise<ReponseModele> {
    // 1. V√©rifier que le mod√®le est pr√™t
    if (!this.estPret()) {
      throw {
        code: 'MODELE_NON_PRET',
        message: 'Le mod√®le doit √™tre charg√© avant de g√©n√©rer du texte'
      } as ErreurWebLLM;
    }

    try {
      const tempsDebut = Date.now();

      // 2. Param√®tres par d√©faut si non fournis
      const paramsFinaux: ParametresGeneration = {
        temperature: parametres?.temperature ?? 0.7,
        longueurMaximale: parametres?.longueurMaximale ?? 100,
        topP: parametres?.topP ?? 0.9,
        penaliteFrequence: parametres?.penaliteFrequence ?? 0.0
      };

      // 3. Convertir nos messages au format WebLLM
      // S'assurer que le message syst√®me est toujours en premier
      const systemMessage = messages.find(msg => msg.role === 'system');
      const otherMessages = messages.filter(msg => msg.role !== 'system');
      
      const sortedMessages = systemMessage 
        ? [systemMessage, ...otherMessages]
        : otherMessages;
      
      const messagesWebLLM = sortedMessages.map(msg => ({
        role: msg.role,
        content: msg.contenu
      }));

      console.log("ü§î G√©n√©ration en cours...");

      // 4. G√©n√©rer le texte avec streaming
      const reponseStream = await this.moteur!.chat.completions.create({
        messages: messagesWebLLM,
        temperature: paramsFinaux.temperature,
        max_tokens: paramsFinaux.longueurMaximale,
        top_p: paramsFinaux.topP,
        frequency_penalty: paramsFinaux.penaliteFrequence,
        stream: true  // Streaming activ√©
      });

      let texteComplet = "";
      let tokensUtilises = 0;
      let lastChunkWithUsage: any = null;

      // Traiter les chunks du stream
      for await (const chunk of reponseStream) {
        const nouveauTexte = chunk.choices[0]?.delta?.content || "";
        
        // Ajouter au texte complet
        texteComplet += nouveauTexte;
        
        // Appeler le callback si fourni (pour l'UI)
        if (onChunk && nouveauTexte) {
          onChunk(nouveauTexte);
        }
        
        // Afficher dans la console pour le d√©bogage
        if (nouveauTexte) {
          console.log("Chunk re√ßu:", nouveauTexte);
        }
        
        // Garder une r√©f√©rence au dernier chunk (qui contient souvent les infos d'usage)
        lastChunkWithUsage = chunk;
      }

      const tempsFin = Date.now();
      const tempsGeneration = tempsFin - tempsDebut;

      // R√©cup√©rer le nombre de tokens depuis le dernier chunk ou l'usage
      if (lastChunkWithUsage?.usage?.total_tokens) {
        tokensUtilises = lastChunkWithUsage.usage.total_tokens;
      } else {
        // Estimation approximative si l'API ne fournit pas l'usage dans le streaming
        tokensUtilises = Math.ceil(texteComplet.length / 4); // Estimation: ~4 caract√®res par token
      }

      console.log(`‚úÖ Texte g√©n√©r√© en ${tempsGeneration}ms`);
      console.log(`Longueur totale: ${texteComplet.length} caract√®res`);
      console.log(`Tokens estim√©s: ${tokensUtilises}`);

      // 5. Retourner la r√©ponse format√©e
      return {
        texte: texteComplet,
        tokensUtilises,
        tempsGeneration
      };

    } catch (erreur) {
      console.error("‚ùå Erreur lors de la g√©n√©ration :", erreur);
      
      throw {
        code: 'ERREUR_GENERATION',
        message: 'Erreur lors de la g√©n√©ration du texte',
        details: erreur instanceof Error ? erreur.message : String(erreur)
      } as ErreurWebLLM;
    }
  }


  /**
   * D√©charger le mod√®le de la m√©moire
   * Utile pour lib√©rer de la RAM
   */
  public async dechargerModele(): Promise<void> {
    if (this.moteur) {
      console.log("üóëÔ∏è D√©chargement du mod√®le...");
      // Note: WebLLM n'a pas de m√©thode explicite de d√©chargement
      // On met juste √† null pour permettre au garbage collector de nettoyer
      this.moteur = null;
      this.changerStatut('inactif');
      this.configuration = null;
      console.log("‚úÖ Mod√®le d√©charg√©");
    }
  }

  // ============================================
  // M√âTHODES PRIV√âES (Helpers internes)
  // ============================================

  /**
   * Changer le statut et notifier les observateurs
   */
  private changerStatut(nouveauStatut: StatutModele): void {
    this.statut = nouveauStatut;
    if (this.observateurs.surChangementStatut) {
      this.observateurs.surChangementStatut(nouveauStatut);
    }
  }

  /**
   * Notifier la progression du chargement
   */
  private notifierProgression(progression: ProgressionChargement): void {
    if (this.observateurs.surProgression) {
      this.observateurs.surProgression(progression);
    }
  }

  /**
   * Notifier une erreur
   */
  private notifierErreur(erreur: ErreurWebLLM): void {
    if (this.observateurs.surErreur) {
      this.observateurs.surErreur(erreur);
    }
  }
}

// Exporter une instance unique (Singleton)
export const serviceMoteur = ServiceMoteurWebLLM.obtenirInstance();