// src/lib/webllm/moteur.ts
// Service principal pour gÃ©rer WebLLM - Pattern Singleton

import { CreateMLCEngine, MLCEngine } from "@mlc-ai/web-llm";
//import { appConfig } from "./appConfig";
import type {
  StatutModele,
  ProgressionChargement,
  ConfigurationModele,
  ParametresGeneration,
  Message,
  ReponseModele,
  ErreurWebLLM
} from './types';


// ou
//console.log(MLCEngine);

/**
 * Service Singleton pour gÃ©rer le modÃ¨le WebLLM
 * 
 * ResponsabilitÃ©s :
 * - Charger le modÃ¨le une seule fois
 * - GÃ©rer le statut du modÃ¨le
 * - GÃ©nÃ©rer du texte
 * - GÃ©rer les erreurs
 * 
 * Pattern Singleton : Une seule instance dans toute l'application
 */
class ServiceMoteurWebLLM {

  private static instance: ServiceMoteurWebLLM | null = null;   // Instance unique du service (Singleton)

  private moteur: MLCEngine | null = null;   // Le moteur WebLLM (null tant qu'il n'est pas chargÃ©)
 
  private statut: StatutModele = 'inactif';  // Statut actuel du modÃ¨le
  
  private configuration: ConfigurationModele | null = null; // Configuration du modÃ¨le actuellement chargÃ©
  
  // Callbacks pour notifier les changements de statut
  private observateurs: {
    surChangementStatut?: (statut: StatutModele) => void;
    surProgression?: (progression: ProgressionChargement) => void;
    surErreur?: (erreur: ErreurWebLLM) => void;
  } = {};

  /**
   * Constructeur privÃ© (Singleton)
   * Ne peut pas Ãªtre appelÃ© directement
   */
  private constructor() {
    console.log("ğŸ¤– Service WebLLM initialisÃ©");
  }

  /**
   * Obtenir l'instance unique du service (Singleton)
   */
  public static obtenirInstance(): ServiceMoteurWebLLM {
    if (!ServiceMoteurWebLLM.instance) {
      ServiceMoteurWebLLM.instance = new ServiceMoteurWebLLM();
    }
    return ServiceMoteurWebLLM.instance;
  }

  /**
   * Enregistrer des callbacks pour Ãªtre notifiÃ© des changements
   */
  public enregistrerObservateurs(callbacks: {
    surChangementStatut?: (statut: StatutModele) => void;
    surProgression?: (progression: ProgressionChargement) => void;
    surErreur?: (erreur: ErreurWebLLM) => void;
  }): void {
    this.observateurs = { ...this.observateurs, ...callbacks };
  }

  /**
   * Obtenir le statut actuel du modÃ¨le
   */
  public obtenirStatut(): StatutModele {
    return this.statut;
  }

  /**
   * VÃ©rifier si le modÃ¨le est prÃªt Ã  gÃ©nÃ©rer du texte
   */
  public estPret(): boolean {
    return this.statut === 'pret' && this.moteur !== null;
  }

  /**
   * Charger le modÃ¨le WebLLM
   */
  /**
   * Charger le modÃ¨le WebLLM
   */
  public async chargerModele(config: ConfigurationModele): Promise<void> {
    try {
      // 1. VÃ©rifier si un modÃ¨le est dÃ©jÃ  en cours de chargement
      if (this.statut === 'chargement') {
        console.warn("âš ï¸ Un modÃ¨le est dÃ©jÃ  en cours de chargement");
        return;
      }

      // 2. Mettre Ã  jour le statut
      this.changerStatut('chargement');
      this.configuration = config;

      console.log(`ğŸ”„ DÃ©but du chargement du modÃ¨le : ${config.nom}`);

      // Variable pour tracer le dernier pourcentage affichÃ©
      let dernierPourcentage = 0;

      // 3. CrÃ©er le moteur WebLLM avec suivi de progression + appConfig
      this.moteur = await CreateMLCEngine(
        config.nom,
        {
         // appConfig, // â† UTILISATION DE appConfig ICI
          // Callback appelÃ© pendant le chargement
          initProgressCallback: (rapport) => {
            const pourcentage = Math.round(rapport.progress * 100);
            
            // Afficher seulement tous les 10% ou Ã  100%
            if (pourcentage >= dernierPourcentage + 10 || pourcentage === 100) {
              console.log(`â³ ${pourcentage}% - ${rapport.text}`);
              dernierPourcentage = pourcentage;
            }
            
            // Notifier les observateurs (garde la progression exacte pour l'UI)
            this.notifierProgression({
              pourcentage: rapport.progress * 100,
              etape: rapport.text
            });
          }
        }
      );

      // 4. ModÃ¨le chargÃ© avec succÃ¨s !
      console.log("âœ… ModÃ¨le chargÃ© avec succÃ¨s !");
      this.changerStatut('pret');

    } catch (erreur) {
      // 5. GÃ©rer les erreurs
      console.error("âŒ Erreur lors du chargement du modÃ¨le :", erreur);
      
      const erreurFormatee: ErreurWebLLM = {
        code: 'ERREUR_CHARGEMENT',
        message: "Impossible de charger le modÃ¨le",
        details: erreur instanceof Error ? erreur.message : String(erreur)
      };

      this.changerStatut('erreur');
      this.notifierErreur(erreurFormatee);
      
      throw erreurFormatee;
    }
  }
 


  /**
   * GÃ©nÃ©rer du texte avec le modÃ¨le
   */
public async genererTexte(
  messages: Message[],
  parametres?: ParametresGeneration,
  onChunk?: (chunk: string) => void,   
): Promise<ReponseModele> {
  // 1. VÃ©rifier que le modÃ¨le est prÃªt
  if (!this.estPret()) {
    throw {
      code: 'MODELE_NON_PRET',
      message: 'Le modÃ¨le doit Ãªtre chargÃ© avant de gÃ©nÃ©rer du texte'
    } as ErreurWebLLM;
  }

  try {
    const tempsDebut = Date.now();
    
    // ============================================
    // ğŸ“Š LOGS AMÃ‰LIORÃ‰S POUR LE DEBUG
    // ============================================
    console.log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    console.log("ğŸš€ MOTEUR : DÃ©but gÃ©nÃ©ration");
    console.log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    
    const contenuSysteme = messages[0]?.contenu || '';
    
    // DÃ©tection de l'action
    const actionMatch = contenuSysteme.match(/Action : (corrige|amÃ©liore|raccourcis|allonge)/i);
    const actionDetectee = actionMatch ? actionMatch[1] : 'inconnue';
    
    // VÃ©rification structure
    const hasBalises = contenuSysteme.includes('<TexteUtilisateur>');
    const hasInstructionsReponse = contenuSysteme.includes('Comment rÃ©pondre :');
    
   // console.log(`ğŸ“¤ Action dÃ©tectÃ©e: ${actionDetectee}`);
  //  console.log(`ğŸ“ Longueur prompt: ${contenuSysteme.length} caractÃ¨res`);
    //console.log(`ğŸ·ï¸  Balises TexteUtilisateur: ${hasBalises ? 'âœ…' : 'âŒ'}`);
  //  console.log(`ğŸ“ Instructions rÃ©ponse: ${hasInstructionsReponse ? 'âœ…' : 'âŒ'}`);
    
    // Extraire le texte utilisateur pour info
    const texteMatch = contenuSysteme.match(/<TexteUtilisateur>\n([\s\S]*?)\n<\/TexteUtilisateur>/);
    if (texteMatch) {
      const texteUser = texteMatch[1];
      console.log(`ğŸ“„ Texte utilisateur: ${texteUser.substring(0, 50)}... (${texteUser.length} caractÃ¨res)`);
    }

    // 2. ParamÃ¨tres par dÃ©faut
    const paramsFinaux: ParametresGeneration = {
      temperature: parametres?.temperature ?? 0.7,
      longueurMaximale: parametres?.longueurMaximale ?? 1000,
      topP: parametres?.topP ?? 0.9,
      penaliteFrequence: parametres?.penaliteFrequence ?? 0.0
    };

    console.log(`âš™ï¸ ParamÃ¨tres: temp=${paramsFinaux.temperature}, max_tokens=${paramsFinaux.longueurMaximale}`);

    // 3. Convertir nos messages au format WebLLM
    // IMPORTANT: Garder l'ordre systÃ¨me puis utilisateur
    const messagesWebLLM = messages.map(msg => ({
      role: msg.role,
      content: msg.contenu
    }));

    console.log("ğŸ”„ GÃ©nÃ©ration en cours...");

    // 4. GÃ©nÃ©rer le texte avec streaming
    const reponseStream = await this.moteur!.chat.completions.create({
      messages: messagesWebLLM,
      temperature: paramsFinaux.temperature,
      max_tokens: paramsFinaux.longueurMaximale,
      top_p: paramsFinaux.topP,
      frequency_penalty: paramsFinaux.penaliteFrequence,
      stream: true
    });

    let texteComplet = "";
    let tokensUtilises = 0;
    let lastChunkWithUsage: any = null;
    let chunkCount = 0;

    // Traiter les chunks du stream
    for await (const chunk of reponseStream) {
      const nouveauTexte = chunk.choices[0]?.delta?.content || "";
      texteComplet += nouveauTexte;
      chunkCount++;
      
      if (onChunk && nouveauTexte) {
        onChunk(nouveauTexte);
      }
      
      lastChunkWithUsage = chunk;
    }

    const tempsFin = Date.now();
    const tempsGeneration = tempsFin - tempsDebut;

    // RÃ©cupÃ©rer le nombre de tokens
    if (lastChunkWithUsage?.usage?.total_tokens) {
      tokensUtilises = lastChunkWithUsage.usage.total_tokens;
    } else {
      tokensUtilises = Math.ceil(texteComplet.length / 4);
    }

    // ============================================
    // ğŸ“Š LOGS DE RÃ‰SULTAT
    // ============================================
    /*console.log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    console.log("âœ… GÃ‰NÃ‰RATION TERMINÃ‰E");
    console.log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    console.log(`â±ï¸  Temps: ${tempsGeneration}ms`);
    console.log(`ğŸ“ RÃ©ponse: ${texteComplet.length} caractÃ¨res`);
    console.log(`ğŸ¯ Tokens estimÃ©s: ${tokensUtilises}`);
    console.log(`ğŸ”„ Chunks reÃ§us: ${chunkCount}`);
    console.log(`ğŸ“ AperÃ§u rÃ©ponse: "${texteComplet.substring(0, 100)}..."`);
    console.log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");*/

    // 5. Retourner la rÃ©ponse formatÃ©e
    return {
      texte: texteComplet,
      tokensUtilises,
      tempsGeneration
    };

  } catch (erreur) {
    console.error("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    console.error("âŒ ERREUR LORS DE LA GÃ‰NÃ‰RATION");
    console.error("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    console.error("DÃ©tails:", erreur);
    
    throw {
      code: 'ERREUR_GENERATION',
      message: 'Erreur lors de la gÃ©nÃ©ration du texte',
      details: erreur instanceof Error ? erreur.message : String(erreur)
    } as ErreurWebLLM;
  }
}

  /**
   * DÃ©charger le modÃ¨le de la mÃ©moire
   */
  public async dechargerModele(): Promise<void> {
    if (this.moteur) {
      console.log("ğŸ—‘ï¸ DÃ©chargement du modÃ¨le...");
      this.moteur = null;
      this.changerStatut('inactif');
      this.configuration = null;
      console.log("âœ… ModÃ¨le dÃ©chargÃ©");
    }
  }

  // ============================================
  // MÃ‰THODES PRIVÃ‰ES (Helpers internes)
  // ============================================

  /**
   * Changer le statut et notifier les observateurs
   */
  private changerStatut(nouveauStatut: StatutModele): void {
    this.statut = nouveauStatut;
    if (this.observateurs.surChangementStatut) {
      this.observateurs.surChangementStatut(nouveauStatut);
    }
  }

  /**
   * Notifier la progression du chargement
   */
  private notifierProgression(progression: ProgressionChargement): void {
    if (this.observateurs.surProgression) {
      this.observateurs.surProgression(progression);
    }
  }

  /**
   * Notifier une erreur
   */
  private notifierErreur(erreur: ErreurWebLLM): void {
    if (this.observateurs.surErreur) {
      this.observateurs.surErreur(erreur);
    }
  }
}

// Exporter une instance unique (Singleton)
export const serviceMoteur = ServiceMoteurWebLLM.obtenirInstance();
