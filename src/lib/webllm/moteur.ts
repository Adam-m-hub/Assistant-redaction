// src/lib/webllm/moteur.ts
// Service principal pour g√©rer WebLLM - Pattern Singleton

import { CreateMLCEngine, MLCEngine } from "@mlc-ai/web-llm";
import type {
  StatutModele,
  ProgressionChargement,
  ConfigurationModele,
  ParametresGeneration,
  Message,
  ReponseModele,
  ErreurWebLLM
} from './types';

/**
 * Service Singleton pour g√©rer le mod√®le WebLLM
 * 
 * Responsabilit√©s :
 * - Charger le mod√®le une seule fois
 * - G√©rer le statut du mod√®le
 * - G√©n√©rer du texte avec streaming
 * - G√©rer les erreurs
 */
class ServiceMoteurWebLLM {
  private static instance: ServiceMoteurWebLLM | null = null;
  private moteur: MLCEngine | null = null;
  private statut: StatutModele = 'inactif';
  private configuration: ConfigurationModele | null = null;
  
  // Callbacks pour notifier les changements
  private observateurs: {
    surChangementStatut?: (statut: StatutModele) => void;
    surProgression?: (progression: ProgressionChargement) => void;
    surErreur?: (erreur: ErreurWebLLM) => void;
  } = {};

  /**
   * Constructeur priv√© (Singleton)
   */
  private constructor() {
    console.log("ü§ñ Service WebLLM initialis√©");
  }

  /**
   * Obtenir l'instance unique (Singleton)
   */
  public static obtenirInstance(): ServiceMoteurWebLLM {
    if (!ServiceMoteurWebLLM.instance) {
      ServiceMoteurWebLLM.instance = new ServiceMoteurWebLLM();
    }
    return ServiceMoteurWebLLM.instance;
  }

  /**
   * Enregistrer des callbacks
   */
  public enregistrerObservateurs(callbacks: {
    surChangementStatut?: (statut: StatutModele) => void;
    surProgression?: (progression: ProgressionChargement) => void;
    surErreur?: (erreur: ErreurWebLLM) => void;
  }): void {
    this.observateurs = { ...this.observateurs, ...callbacks };
  }

  /**
   * Obtenir le statut actuel
   */
  public obtenirStatut(): StatutModele {
    return this.statut;
  }

  /**
   * V√©rifier si le mod√®le est pr√™t
   */
  public estPret(): boolean {
    return this.statut === 'pret' && this.moteur !== null;
  }

  /**
   * Charger le mod√®le WebLLM
   */
  public async chargerModele(config: ConfigurationModele): Promise<void> {
    try {
      // V√©rifier si d√©j√† en cours
      if (this.statut === 'chargement') {
        console.warn("‚ö†Ô∏è Un mod√®le est d√©j√† en cours de chargement");
        return;
      }

      this.changerStatut('chargement');
      this.configuration = config;

      console.log(`üîÑ Chargement : ${config.nom}`);

      let dernierPourcentage = 0;

      // Cr√©er le moteur avec suivi de progression
      this.moteur = await CreateMLCEngine(
        config.nom,
        {
          initProgressCallback: (rapport) => {
            const pourcentage = Math.round(rapport.progress * 100);
            
            // Log tous les 10% ou √† 100%
            if (pourcentage >= dernierPourcentage + 10 || pourcentage === 100) {
              console.log(`‚è≥ ${pourcentage}% - ${rapport.text}`);
              dernierPourcentage = pourcentage;
            }
            
            // Notifier les observateurs
            this.notifierProgression({
              pourcentage: rapport.progress * 100,
              etape: rapport.text
            });
          }
        }
      );

      console.log("‚úÖ Mod√®le charg√© !");
      this.changerStatut('pret');

    } catch (erreur) {
      console.error("‚ùå Erreur chargement :", erreur);
      
      const erreurFormatee: ErreurWebLLM = {
        code: 'ERREUR_CHARGEMENT',
        message: "Impossible de charger le mod√®le",
        details: erreur instanceof Error ? erreur.message : String(erreur)
      };

      this.changerStatut('erreur');
      this.notifierErreur(erreurFormatee);
      
      throw erreurFormatee;
    }
  }

  /**
   * G√©n√©rer du texte avec streaming
   */
  public async genererTexte(
    messages: Message[],
    parametres?: ParametresGeneration,
    onChunk?: (chunk: string) => void,   
  ): Promise<ReponseModele> {
    // V√©rifier que le mod√®le est pr√™t
    if (!this.estPret()) {
      throw {
        code: 'MODELE_NON_PRET',
        message: 'Le mod√®le doit √™tre charg√© avant de g√©n√©rer du texte'
      } as ErreurWebLLM;
    }

    try {
      const tempsDebut = Date.now();

      // Param√®tres par d√©faut
      const paramsFinaux: ParametresGeneration = {
        temperature: parametres?.temperature ?? 0.7,
        longueurMaximale: parametres?.longueurMaximale ?? 1000,
        topP: parametres?.topP ?? 0.9,
        penaliteFrequence: parametres?.penaliteFrequence ?? 0.0
      };

      // Convertir au format WebLLM
      const messagesWebLLM = messages.map(msg => ({
        role: msg.role,
        content: msg.contenu
      }));

     // console.log(`üöÄ G√©n√©ration (max: ${paramsFinaux.longueurMaximale} tokens)`);

      // G√©n√©rer avec streaming
      const reponseStream = await this.moteur!.chat.completions.create({
        messages: messagesWebLLM,
        temperature: paramsFinaux.temperature,
        max_tokens: paramsFinaux.longueurMaximale,
        top_p: paramsFinaux.topP,
        frequency_penalty: paramsFinaux.penaliteFrequence,
        stream: true
      });

      let texteComplet = "";
      let tokensUtilises = 0;
      let lastChunk: any = null;

      // Traiter les chunks
      for await (const chunk of reponseStream) {
        const nouveauTexte = chunk.choices[0]?.delta?.content || "";
        texteComplet += nouveauTexte;
        
        if (onChunk && nouveauTexte) {
          onChunk(nouveauTexte);
        }
        
        lastChunk = chunk;
      }

      const tempsFin = Date.now();
      const tempsGeneration = tempsFin - tempsDebut;

      // R√©cup√©rer les tokens
      tokensUtilises = lastChunk?.usage?.total_tokens 
        ?? Math.ceil(texteComplet.length / 4);

    //  console.log(`‚úÖ G√©n√©r√© : ${texteComplet.length} caract√®res en ${tempsGeneration}ms`);

      return {
        texte: texteComplet,
        tokensUtilises,
        tempsGeneration
      };

    } catch (erreur) {
      console.error("‚ùå Erreur g√©n√©ration :", erreur);
      
      throw {
        code: 'ERREUR_GENERATION',
        message: 'Erreur lors de la g√©n√©ration du texte',
        details: erreur instanceof Error ? erreur.message : String(erreur)
      } as ErreurWebLLM;
    }
  }

  /**
   * D√©charger le mod√®le
   */
  public async dechargerModele(): Promise<void> {
    if (this.moteur) {
    //  console.log("üóëÔ∏è D√©chargement...");
      this.moteur = null;
      this.changerStatut('inactif');
      this.configuration = null;
    //  console.log("‚úÖ D√©charg√©");
    }
  }

  // ============================================
  // M√âTHODES PRIV√âES
  // ============================================

  private changerStatut(nouveauStatut: StatutModele): void {
    this.statut = nouveauStatut;
    if (this.observateurs.surChangementStatut) {
      this.observateurs.surChangementStatut(nouveauStatut);
    }
  }

  private notifierProgression(progression: ProgressionChargement): void {
    if (this.observateurs.surProgression) {
      this.observateurs.surProgression(progression);
    }
  }

  private notifierErreur(erreur: ErreurWebLLM): void {
    if (this.observateurs.surErreur) {
      this.observateurs.surErreur(erreur);
    }
  }
}

// Exporter l'instance unique (Singleton)
export const serviceMoteur = ServiceMoteurWebLLM.obtenirInstance();
