// src/lib/webllm/moteur.ts
// Service principal pour g√©rer WebLLM - Pattern Singleton

import { CreateMLCEngine, MLCEngine } from "@mlc-ai/web-llm";
//import { appConfig } from "./appConfig";
import type {
  StatutModele,
  ProgressionChargement,
  ConfigurationModele,
  ParametresGeneration,
  Message,
  ReponseModele,
  ErreurWebLLM
} from './types';


// ou
//console.log(MLCEngine);

/**
 * Service Singleton pour g√©rer le mod√®le WebLLM
 * 
 * Responsabilit√©s :
 * - Charger le mod√®le une seule fois
 * - G√©rer le statut du mod√®le
 * - G√©n√©rer du texte
 * - G√©rer les erreurs
 * 
 * Pattern Singleton : Une seule instance dans toute l'application
 */
class ServiceMoteurWebLLM {

  private static instance: ServiceMoteurWebLLM | null = null;   // Instance unique du service (Singleton)

  private moteur: MLCEngine | null = null;   // Le moteur WebLLM (null tant qu'il n'est pas charg√©)
 
  private statut: StatutModele = 'inactif';  // Statut actuel du mod√®le
  
  private configuration: ConfigurationModele | null = null; // Configuration du mod√®le actuellement charg√©
  
  // Callbacks pour notifier les changements de statut
  private observateurs: {
    surChangementStatut?: (statut: StatutModele) => void;
    surProgression?: (progression: ProgressionChargement) => void;
    surErreur?: (erreur: ErreurWebLLM) => void;
  } = {};

  /**
   * Constructeur priv√© (Singleton)
   * Ne peut pas √™tre appel√© directement
   */
  private constructor() {
    console.log("ü§ñ Service WebLLM initialis√©");
  }

  /**
   * Obtenir l'instance unique du service (Singleton)
   */
  public static obtenirInstance(): ServiceMoteurWebLLM {
    if (!ServiceMoteurWebLLM.instance) {
      ServiceMoteurWebLLM.instance = new ServiceMoteurWebLLM();
    }
    return ServiceMoteurWebLLM.instance;
  }

  /**
   * Enregistrer des callbacks pour √™tre notifi√© des changements
   */
  public enregistrerObservateurs(callbacks: {
    surChangementStatut?: (statut: StatutModele) => void;
    surProgression?: (progression: ProgressionChargement) => void;
    surErreur?: (erreur: ErreurWebLLM) => void;
  }): void {
    this.observateurs = { ...this.observateurs, ...callbacks };
  }

  /**
   * Obtenir le statut actuel du mod√®le
   */
  public obtenirStatut(): StatutModele {
    return this.statut;
  }

  /**
   * V√©rifier si le mod√®le est pr√™t √† g√©n√©rer du texte
   */
  public estPret(): boolean {
    return this.statut === 'pret' && this.moteur !== null;
  }

  /**
   * Charger le mod√®le WebLLM
   */
  /**
   * Charger le mod√®le WebLLM
   */
  public async chargerModele(config: ConfigurationModele): Promise<void> {
    try {
      // 1. V√©rifier si un mod√®le est d√©j√† en cours de chargement
      if (this.statut === 'chargement') {
        console.warn("‚ö†Ô∏è Un mod√®le est d√©j√† en cours de chargement");
        return;
      }

      // 2. Mettre √† jour le statut
      this.changerStatut('chargement');
      this.configuration = config;

      console.log(`üîÑ D√©but du chargement du mod√®le : ${config.nom}`);

      // Variable pour tracer le dernier pourcentage affich√©
      let dernierPourcentage = 0;

      // 3. Cr√©er le moteur WebLLM avec suivi de progression + appConfig
      this.moteur = await CreateMLCEngine(
        config.nom,
        {
         // appConfig, // ‚Üê UTILISATION DE appConfig ICI
          // Callback appel√© pendant le chargement
          initProgressCallback: (rapport) => {
            const pourcentage = Math.round(rapport.progress * 100);
            
            // Afficher seulement tous les 10% ou √† 100%
            if (pourcentage >= dernierPourcentage + 10 || pourcentage === 100) {
              console.log(`‚è≥ ${pourcentage}% - ${rapport.text}`);
              dernierPourcentage = pourcentage;
            }
            
            // Notifier les observateurs (garde la progression exacte pour l'UI)
            this.notifierProgression({
              pourcentage: rapport.progress * 100,
              etape: rapport.text
            });
          }
        }
      );

      // 4. Mod√®le charg√© avec succ√®s !
      console.log("‚úÖ Mod√®le charg√© avec succ√®s !");
      this.changerStatut('pret');

    } catch (erreur) {
      // 5. G√©rer les erreurs
      console.error("‚ùå Erreur lors du chargement du mod√®le :", erreur);
      
      const erreurFormatee: ErreurWebLLM = {
        code: 'ERREUR_CHARGEMENT',
        message: "Impossible de charger le mod√®le",
        details: erreur instanceof Error ? erreur.message : String(erreur)
      };

      this.changerStatut('erreur');
      this.notifierErreur(erreurFormatee);
      
      throw erreurFormatee;
    }
  }
/*
// Dans moteur.ts - REMPLACE toute la fonction chargerModele
public async chargerModele(config: ConfigurationModele): Promise<void> {
  try {
    if (this.statut === 'chargement') return;
    
    this.changerStatut('chargement');
    this.configuration = config;

    console.log(`üîÑ Chargement du mod√®le : ${config.nom}`);

    let dernierPourcentage = 0;

    // NOUVELLE M√âTHODE : utilise l'URL CDN directe
    this.moteur = await CreateMLCEngine(
      config.nom,
      {
        // WebLLM va chercher automatiquement
        initProgressCallback: (rapport) => {
          const pourcentage = Math.round(rapport.progress * 100);
          
          if (pourcentage >= dernierPourcentage + 10 || pourcentage === 100) {
            console.log(`‚è≥ ${pourcentage}% - ${rapport.text}`);
            dernierPourcentage = pourcentage;
          }
          
          this.notifierProgression({
            pourcentage: rapport.progress * 100,
            etape: rapport.text
          });
        }
      }
    );

    console.log("‚úÖ Mod√®le charg√© avec succ√®s !");
    this.changerStatut('pret');

  } catch (erreur) {
    console.error("‚ùå Erreur :", erreur);
    
    // Si √©chec, essaie avec un mod√®le plus simple
    if (config.nom.includes("Llama")) {
      console.log("üîÑ Essaie avec TinyLlama √† la place...");
      // Essaie automatiquement avec TinyLlama
      await this.chargerModele({
        nom: "TinyLlama-1.1B-Chat-v1.0-q4f16_1",
        tailleMemoire: 512,
        description: "TinyLlama (backup)"
      });
      return;
    }
    
    this.changerStatut('erreur');
    this.notifierErreur({
      code: 'ERREUR_CHARGEMENT',
      message: "Impossible de charger le mod√®le",
      details: erreur instanceof Error ? erreur.message : String(erreur)
    });
  }
}*/


  /**
   * G√©n√©rer du texte avec le mod√®le
   */
  public async genererTexte(
    messages: Message[],
    parametres?: ParametresGeneration,
    onChunk?: (chunk: string) => void,   
  ): Promise<ReponseModele> {
    // 1. V√©rifier que le mod√®le est pr√™t
    if (!this.estPret()) {
      throw {
        code: 'MODELE_NON_PRET',
        message: 'Le mod√®le doit √™tre charg√© avant de g√©n√©rer du texte'
      } as ErreurWebLLM;
    }

    try {
      const tempsDebut = Date.now();
      
      // üìä CONSOLE LOG - Messages envoy√©s au mod√®le WebLLM
      console.log("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ");
      console.log("üì§ MOTEUR : Envoi au mod√®le WebLLM");
      console.log("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ");
      messages.forEach((msg, index) => {
        console.log(`\n[Message ${index + 1}] ${msg.role.toUpperCase()}:`);
        console.log(msg.contenu);
      });
      console.log("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ");

      // 2. Param√®tres par d√©faut si non fournis
      const paramsFinaux: ParametresGeneration = {
        temperature: parametres?.temperature ?? 0.7,
        longueurMaximale: parametres?.longueurMaximale ?? 1000,
        topP: parametres?.topP ?? 0.9,
        penaliteFrequence: parametres?.penaliteFrequence ?? 0.0
      };

      console.log("‚öôÔ∏è Param√®tres WebLLM :", paramsFinaux);

      // 3. Convertir nos messages au format WebLLM
      // S'assurer que le message syst√®me est toujours en premier
      const systemMessage = messages.find(msg => msg.role === 'system');
      const otherMessages = messages.filter(msg => msg.role !== 'system');
      
      const sortedMessages = systemMessage 
        ? [systemMessage, ...otherMessages]
        : otherMessages;
      
      const messagesWebLLM = sortedMessages.map(msg => ({
        role: msg.role,
        content: msg.contenu
      }));

      console.log("üîÑ G√©n√©ration en cours...");

      // 4. G√©n√©rer le texte avec streaming
      const reponseStream = await this.moteur!.chat.completions.create({
        messages: messagesWebLLM,
        temperature: paramsFinaux.temperature,
        max_tokens: paramsFinaux.longueurMaximale,
        top_p: paramsFinaux.topP,
        frequency_penalty: paramsFinaux.penaliteFrequence,
        stream: true  // Streaming activ√©
      });

      let texteComplet = "";
      let tokensUtilises = 0;
      let lastChunkWithUsage: any = null;

      // Traiter les chunks du stream
      for await (const chunk of reponseStream) {
        const nouveauTexte = chunk.choices[0]?.delta?.content || "";
        
        // Ajouter au texte complet
        texteComplet += nouveauTexte;
        
        // Appeler le callback si fourni (pour l'UI)
        if (onChunk && nouveauTexte) {
          onChunk(nouveauTexte);
        }
        
        // Garder une r√©f√©rence au dernier chunk
        lastChunkWithUsage = chunk;
      }

      const tempsFin = Date.now();
      const tempsGeneration = tempsFin - tempsDebut;

      // R√©cup√©rer le nombre de tokens
      if (lastChunkWithUsage?.usage?.total_tokens) {
        tokensUtilises = lastChunkWithUsage.usage.total_tokens;
      } else {
        // Estimation approximative
        tokensUtilises = Math.ceil(texteComplet.length / 4);
      }

      //console.log(`‚úÖ Texte g√©n√©r√© en ${tempsGeneration}ms`);
      //console.log(`üìè Longueur : ${texteComplet.length} caract√®res`);
      //console.log(`üéØ Tokens : ${tokensUtilises}`);
      //console.log(`üìù Aper√ßu : ${texteComplet.substring(0, 100)}...`);

      // 5. Retourner la r√©ponse format√©e
      return {
        texte: texteComplet,
        tokensUtilises,
        tempsGeneration
      };

    } catch (erreur) {
      console.error("‚ùå Erreur lors de la g√©n√©ration :", erreur);
      
      throw {
        code: 'ERREUR_GENERATION',
        message: 'Erreur lors de la g√©n√©ration du texte',
        details: erreur instanceof Error ? erreur.message : String(erreur)
      } as ErreurWebLLM;
    }
  }

  /**
   * D√©charger le mod√®le de la m√©moire
   */
  public async dechargerModele(): Promise<void> {
    if (this.moteur) {
      console.log("üóëÔ∏è D√©chargement du mod√®le...");
      this.moteur = null;
      this.changerStatut('inactif');
      this.configuration = null;
      console.log("‚úÖ Mod√®le d√©charg√©");
    }
  }

  // ============================================
  // M√âTHODES PRIV√âES (Helpers internes)
  // ============================================

  /**
   * Changer le statut et notifier les observateurs
   */
  private changerStatut(nouveauStatut: StatutModele): void {
    this.statut = nouveauStatut;
    if (this.observateurs.surChangementStatut) {
      this.observateurs.surChangementStatut(nouveauStatut);
    }
  }

  /**
   * Notifier la progression du chargement
   */
  private notifierProgression(progression: ProgressionChargement): void {
    if (this.observateurs.surProgression) {
      this.observateurs.surProgression(progression);
    }
  }

  /**
   * Notifier une erreur
   */
  private notifierErreur(erreur: ErreurWebLLM): void {
    if (this.observateurs.surErreur) {
      this.observateurs.surErreur(erreur);
    }
  }
}

// Exporter une instance unique (Singleton)
export const serviceMoteur = ServiceMoteurWebLLM.obtenirInstance();
